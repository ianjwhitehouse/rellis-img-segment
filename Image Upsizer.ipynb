{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d500e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SPLIT = 0 #-1\n",
    "NUM_OF_CLASSES = 35\n",
    "IMAGE_SIZE_REDUCE_FACTOR = 3\n",
    "PREV_MODEL_NAME = \"segformerb3\"\n",
    "\n",
    "# Hyperparams \n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.00001\n",
    "NUM_OF_INTERMEDIATE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f9cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "import numpy as np\n",
    "\n",
    "# files = \"Processed Data/images_\"\n",
    "# with np.load(files + \"%d_predictions_%d_%s.npz\" % (SPLIT, IMAGE_SIZE_REDUCE_FACTOR, PREV_MODEL_NAME)) as data:\n",
    "#     x_train = data[\"train\"].astype(np.float16)\n",
    "#     x_val = data[\"val\"].astype(np.float16)\n",
    "#     x_test = data[\"test\"].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6feae230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ontology\n",
    "import pandas as pd\n",
    "\n",
    "ontology = pd.read_csv(\"Rellis-3D/ontology.csv\")[[\"class_name\", \"output_value\", \"display_color\"]].values.tolist()\n",
    "colors = {v[0]: v[2] for v in ontology}\n",
    "ontology = {v[0]: v[1] for v in ontology}\n",
    "\n",
    "# Remove extra classes\n",
    "del ontology[\"void\"]\n",
    "# del ontology[\"dirt\"]\n",
    "del ontology[\"uphill\"]\n",
    "del ontology[\"downhill\"]\n",
    "\n",
    "# Process colors\n",
    "colors = {c: (int(colors[c][1:3], 16), int(colors[c][3:5], 16), int(colors[c][5:7], 16)) for c in ontology.keys()}\n",
    "\n",
    "# Convert ontology to color map\n",
    "ontology = list(ontology.values())\n",
    "ontology = [i in ontology for i in range(NUM_OF_CLASSES)]\n",
    "    \n",
    "NUM_OF_CLASSES = ontology.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87fc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process y images\n",
    "# from pickle import load\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# def pre_process_images(img):\n",
    "#     return img[1][:, :, ontology]\n",
    "\n",
    "# y_train = load(open(files + \"train_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# # Normalize images\n",
    "# print(\"Processing train\")\n",
    "# with Pool(64) as p:\n",
    "#     y_train = p.map(pre_process_images, y_train)\n",
    "# y_train = np.array(y_train).reshape((len(y_train),) + y_train[0].shape).astype(np.float16)\n",
    "    \n",
    "# y_val = load(open(files + \"val_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# # Normalize images\n",
    "# print(\"Processing val\")\n",
    "# with Pool(64) as p:\n",
    "#     y_val = p.map(pre_process_images, y_val)\n",
    "# y_val = np.array(y_val).reshape((len(y_val),) + y_val[0].shape).astype(np.float16)\n",
    "    \n",
    "# y_test = load(open(files + \"test_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# # Normalize images\n",
    "# print(\"Processing test\")\n",
    "# with Pool(64) as p:\n",
    "#     y_test = p.map(pre_process_images, y_test)\n",
    "# y_test = np.array(y_test).reshape((len(y_test),) + y_test[0].shape).astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00832942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-training data\n",
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "files = \"Processed Data/\"\n",
    "\n",
    "with np.load(files + \"%s_%d.npz\" % (\"train\", SPLIT)) as data:\n",
    "    x_train, y_train = data[\"img_oh_ds\"].astype(np.float16), data[\"img_oh\"].astype(np.float16)\n",
    "    \n",
    "with np.load(files + \"%s_%d.npz\" % (\"val\", SPLIT)) as data:\n",
    "    x_val, y_val = data[\"img_oh_ds\"].astype(np.float16), data[\"img_oh\"].astype(np.float16)\n",
    "\n",
    "with np.load(files + \"%s_%d.npz\" % (\"test\", SPLIT)) as data:\n",
    "    x_test, y_test = data[\"img_oh_ds\"].astype(np.float16), data[\"img_oh\"].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d276cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:40:23.344022: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-31 23:40:23.383933: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-31 23:40:23.928483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# This class streams data to the model\n",
    "# https://stackoverflow.com/a/71592809\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train = DataGenerator(x_train, y_train, BATCH_SIZE)\n",
    "val = DataGenerator(x_val, y_val, BATCH_SIZE)\n",
    "test = DataGenerator(x_test, y_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6883c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0314f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:40:25.599337: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-05-31 23:40:25.599424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22086 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:18:00.0, compute capability: 8.9\n",
      "2023-05-31 23:40:25.600080: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 1\n",
      "2023-05-31 23:40:25.600130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22275 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:af:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Create a distribution strategy\n",
    "# https://www.tensorflow.org/guide/distributed_training\n",
    "\n",
    "# dist_strategy = tf.distribute.experimental.CentralStorageStrategy()\n",
    "# dist_strategy = tf.distribute.MirroredStrategy()\n",
    "dist_strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9cf3249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 400, 640, 38)      6536      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 400, 640, 76)      26068     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 400, 640, 76)      52060     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 400, 640, 76)      52060     \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 1200, 1920, 76)    5852      \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 1200, 1920, 76)    467932    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 1200, 1920, 76)    467932    \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 1200, 1920, 76)    467932    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 1200, 1920, 19)    116983    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1663355 (6.35 MB)\n",
      "Trainable params: 1663355 (6.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create upsizer\n",
    "from tensorflow.keras import layers, models, optimizers, metrics\n",
    "\n",
    "# https://stackoverflow.com/a/71310084 says to put this here\n",
    "# with dist_strategy.scope():\n",
    "upsizer = models.Sequential()\n",
    "\n",
    "# For some reason, this is really slow.  Maybe x_train[0].shape should be in parahentesies?  For some reason the input layer doesnt show up\n",
    "upsizer.add(layers.Input(shape=(x_train[0].shape)))\n",
    "upsizer.add(layers.Conv2D(NUM_OF_CLASSES * 2, 3, activation=\"elu\", padding=\"same\"))\n",
    "\n",
    "for _ in range(NUM_OF_INTERMEDIATE):\n",
    "    upsizer.add(layers.Conv2D(NUM_OF_CLASSES * 4, 3, activation=\"elu\", padding=\"same\"))\n",
    "\n",
    "upsizer.add(layers.Conv2DTranspose(NUM_OF_CLASSES * 4, 1, strides=IMAGE_SIZE_REDUCE_FACTOR, padding=\"same\"))\n",
    "\n",
    "for _ in range(NUM_OF_INTERMEDIATE):\n",
    "    upsizer.add(layers.Conv2D(NUM_OF_CLASSES * 4, 3 * IMAGE_SIZE_REDUCE_FACTOR, activation=\"elu\", padding=\"same\"))\n",
    "\n",
    "upsizer.add(layers.Conv2D(NUM_OF_CLASSES, 3 * IMAGE_SIZE_REDUCE_FACTOR, activation=\"softmax\", padding=\"same\"))\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model_metrics = [\n",
    "    metrics.TopKCategoricalAccuracy(k=1, name='Top 1 Accuracy'),\n",
    "    metrics.TopKCategoricalAccuracy(k=3, name='Top 3 Accuracy'),\n",
    "    metrics.TopKCategoricalAccuracy(k=5, name='Top 5 Accuracy'),\n",
    "    metrics.MeanIoU(num_classes=NUM_OF_CLASSES, sparse_y_true=False, sparse_y_pred=False)\n",
    "]\n",
    "\n",
    "#     upsizer.compile(opt, loss=\"categorical_crossentropy\", metrics=model_metrics)\n",
    "upsizer.compile(opt, loss=\"categorical_focal_crossentropy\", metrics=model_metrics)\n",
    "\n",
    "upsizer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 23:40:26.244529: I tensorflow/core/common_runtime/executor.cc:1210] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "upsizer.fit(\n",
    "    train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = 100,\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],\n",
    "    validation_data = val,\n",
    "    validation_batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e94d4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate metrics on the test set\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "upsizer.evaluate(test)\n",
    "y_pred = upsizer.predict(x_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "reshape_x, reshape_y = y_test.shape[0] * y_test.shape[1] * y_test.shape[2], y_test.shape[3]\n",
    "reshape_x, reshape_y = np.argmax(y_test.reshape(reshape_x, reshape_y), axis=-1), np.argmax(y_pred.reshape(reshape_x, reshape_y), axis=-1)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(reshape_x, reshape_y, target_names=ontology_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "con_matrix = confusion_matrix(reshape_x, reshape_y)\n",
    "con_disp = ConfusionMatrixDisplay(confusion_matrix=con_matrix, display_labels=ontology_labels)\n",
    "con_disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# IoU Per Class\n",
    "false_neg, false_pos = np.sum(con_matrix, axis=0), np.sum(con_matrix, axis=1)\n",
    "print(\"Class\\t\\t\\tIoU\")\n",
    "for i, clas in enumerate(ontology_labels):\n",
    "    tp = con_matrix[i, i]\n",
    "    iou = tp / (tp+ (false_pos[i] - tp) + (false_neg[i] - tp))\n",
    "    print(\"%s\\t\\t\\t%.2f\" % (clas, iou * 100))\n",
    "\n",
    "# Latest run: loss: 0.4106 - Top 1 Accuracy: 0.8647 - Top 3 Accuracy: 0.9856 - Top 5 Accuracy: 0.9921 - mean_io_u: 0.2326\n",
    "\n",
    "# Top runs:\n",
    "# Densenet: loss: 0.4264 - Top 1 Accuracy: 0.8653 - Top 3 Accuracy: 0.9846 - Top 5 Accuracy: 0.9921 - mean_io_u: 0.2334\n",
    "# Nvidia: loss: 0.4958 - Top 1 Accuracy: 0.8499 - Top 3 Accuracy: 0.9796 - Top 5 Accuracy: 0.9917 - mean_io_u: 0.2415\n",
    "# Nvidia b3: loss: 0.3464 - Top 1 Accuracy: 0.8948 - Top 3 Accuracy: 0.9865 - Top 5 Accuracy: 0.9904 - mean_io_u_1: 0.2907"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and save if best\n",
    "from tensorflow import saved_model\n",
    "\n",
    "saved_model.save(upsizer, \"Saved Models/best_upsizer_%s\" % PREV_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f441ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 test images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pre_process_rgb_img(img):\n",
    "    return img[0].astype(np.float16)/255\n",
    "\n",
    "rbg_test = load(open(files + \"train_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# Normalize images\n",
    "print(\"Processing train\")\n",
    "with Pool(64) as p:\n",
    "    rbg_test = p.map(pre_process_rgb_img, rbg_test[:10])\n",
    "rbg_test = np.array(rbg_test).reshape((len(rbg_test),) + rbg_test[0].shape).astype(np.float16)\n",
    "\n",
    "for original, image in zip(y_test[:10, :, :], upsizer.predict(x_test[:10, :, :])):\n",
    "    colored_image = np.argmax(image, axis=-1)\n",
    "    colored_image = np.array(list(colors.values()))[colored_image]\n",
    "    plt.imshow(original.astype(np.float32))\n",
    "    plt.imshow(colored_image, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487403f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd9b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
