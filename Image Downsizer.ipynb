{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d500e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SPLIT = -1\n",
    "NUM_OF_CLASSES = 35\n",
    "IMAGE_SIZE_REDUCE_FACTOR = 3\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87fc6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up processing\n",
    "from pickle import load\n",
    "\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def pre_process_images(img):\n",
    "#     res = (img[0].astype(np.float16)/255, img[1][:, :, ontology])\n",
    "    return (img[0].astype(np.float16)/255, img[1][:, :, ontology])\n",
    "        \n",
    "\n",
    "def split_x_y(data):\n",
    "    x_data = np.array([s[0] for s in data]).reshape((len(data),) + data[0][0].shape)\n",
    "    y_data = np.array([s[1] for s in data]).reshape((len(data),) + data[0][1].shape)    \n",
    "    return x_data, y_data\n",
    "\n",
    "files = \"Processed Data/images_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d276cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 17, 18, 19, 23, 27, 31, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "# Load Ontology\n",
    "import pandas as pd\n",
    "\n",
    "ontology = pd.read_csv(\"Rellis-3D/ontology.csv\")[[\"class_name\", \"output_value\", \"display_color\"]].values.tolist()\n",
    "colors = {v[0]: v[2] for v in ontology}\n",
    "ontology = {v[0]: v[1] for v in ontology}\n",
    "\n",
    "# Remove extra classes\n",
    "del ontology[\"void\"]\n",
    "# del ontology[\"dirt\"]\n",
    "del ontology[\"uphill\"]\n",
    "del ontology[\"downhill\"]\n",
    "\n",
    "# Process colors\n",
    "colors = {c: (int(colors[c][1:3], 16), int(colors[c][3:5], 16), int(colors[c][5:7], 16)) for c in ontology.keys()}\n",
    "\n",
    "# Convert ontology to color map\n",
    "ontology = list(ontology.values())\n",
    "print(ontology)\n",
    "ontology = [i in ontology for i in range(NUM_OF_CLASSES)]\n",
    "    \n",
    "NUM_OF_CLASSES = ontology.count(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759c1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:25:17.632290: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-13 22:25:17.675382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-13 22:25:18.295054: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Disable all GPUS\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "visible_devices = tf.config.get_visible_devices()\n",
    "for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee701bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class streams data to the model\n",
    "# https://stackoverflow.com/a/71592809\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9cf3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downsizer\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model_in = layers.Input(shape=(1200, 1920, 3))\n",
    "x = layers.AveragePooling2D(IMAGE_SIZE_REDUCE_FACTOR)(model_in)\n",
    "\n",
    "x_model = models.Model(model_in, x)\n",
    "x_model.compile('adam', 'mean_squared_error')\n",
    "\n",
    "model_in = layers.Input(shape=(1200, 1920, NUM_OF_CLASSES))\n",
    "x = layers.AveragePooling2D(IMAGE_SIZE_REDUCE_FACTOR)(model_in)\n",
    "\n",
    "y_model = models.Model(model_in, x)\n",
    "y_model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a46a561",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Processed Data/images_train_-1.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess Train\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSPLIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Normalize images\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing images\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Processed Data/images_train_-1.pickle'"
     ]
    }
   ],
   "source": [
    "# Preprocess Train\n",
    "\n",
    "train = load(open(files + \"train_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# Normalize images\n",
    "print(\"Processing images\")\n",
    "with Pool(64) as p:\n",
    "    train = p.map(pre_process_images, train)\n",
    "print(\"Processing done\")\n",
    "\n",
    "print(\"Spliting images\")\n",
    "x_train, y_train = split_x_y(train)\n",
    "del train\n",
    "\n",
    "x_train = DataGenerator(x_train, x_train, BATCH_SIZE)\n",
    "y_train = DataGenerator(y_train, y_train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsize and save train\n",
    "from pickle import dump\n",
    "\n",
    "if IMAGE_SIZE_REDUCE_FACTOR > 1:\n",
    "    x_train = x_model.predict(x_train)\n",
    "    y_train = y_model.predict(y_train)\n",
    "\n",
    "    np.savez(\"Processed Data/images_%s_%d_downsized_%d.npz\" % (\"train\", SPLIT, IMAGE_SIZE_REDUCE_FACTOR), x=x_train, y=y_train)\n",
    "    del x_train, y_train\n",
    "else:\n",
    "    np.savez(\"Processed Data/images_%s_%d_downsized_%d.npz\" % (\"train\", SPLIT, IMAGE_SIZE_REDUCE_FACTOR), x=x_train.x, y=y_train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Val\n",
    "\n",
    "val = load(open(files + \"val_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# Normalize images\n",
    "print(\"Processing images\")\n",
    "with Pool(64) as p:\n",
    "    val = p.map(pre_process_images, val)\n",
    "print(\"Processing done\")\n",
    "\n",
    "print(\"Spliting images\")\n",
    "x_val, y_val = split_x_y(val)\n",
    "del val\n",
    "\n",
    "x_val = DataGenerator(x_val, x_val, BATCH_SIZE)\n",
    "y_val = DataGenerator(y_val, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsize and save val\n",
    "\n",
    "if IMAGE_SIZE_REDUCE_FACTOR > 1:\n",
    "    x_val = x_model.predict(x_val)\n",
    "    y_val = y_model.predict(y_val)\n",
    "\n",
    "    np.savez(\"Processed Data/images_%s_%d_downsized_%d.npz\" % (\"val\", SPLIT, IMAGE_SIZE_REDUCE_FACTOR), x=x_val, y=y_val)\n",
    "    del x_val, y_val\n",
    "else:\n",
    "    np.savez(\"Processed Data/images_%s_%d_downsized_%d.npz\" % (\"val\", SPLIT, IMAGE_SIZE_REDUCE_FACTOR), x=x_val.x, y=y_val.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d44991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Test\n",
    "\n",
    "test = load(open(files + \"test_%d.pickle\" % SPLIT, \"rb\"))\n",
    "\n",
    "# Normalize images\n",
    "print(\"Processing images\")\n",
    "with Pool(64) as p:\n",
    "    test = p.map(pre_process_images, test)\n",
    "print(\"Processing done\")\n",
    "\n",
    "print(\"Spliting images\")\n",
    "x_test, y_test = split_x_y(test)\n",
    "del test\n",
    "\n",
    "x_test = DataGenerator(x_test, x_test, BATCH_SIZE)\n",
    "y_test = DataGenerator(y_test, y_test, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsize and save val\n",
    "\n",
    "if IMAGE_SIZE_REDUCE_FACTOR > 1:\n",
    "    x_test = x_model.predict(x_test)\n",
    "    y_test = y_model.predict(y_test)\n",
    "\n",
    "    np.savez(\"Processed Data/images_%s_%d_downsized_%d.npz\" % (\"test\", SPLIT, IMAGE_SIZE_REDUCE_FACTOR), x=x_test, y=y_test)\n",
    "    del x_test, y_test\n",
    "else:\n",
    "    np.savez(\"Processed Data/images_%s_%d_downsized_%d.npz\" % (\"test\", SPLIT, IMAGE_SIZE_REDUCE_FACTOR), x=np.array(x_test.x), y=np.array(y_test.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e94d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
