{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12ca699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T02:14:34.465528Z",
     "iopub.status.busy": "2023-05-29T02:14:34.464472Z",
     "iopub.status.idle": "2023-05-29T02:14:34.531215Z",
     "shell.execute_reply": "2023-05-29T02:14:34.529816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load image splits\n",
    "from random import shuffle\n",
    "\n",
    "IMAGE_SPLIT_FILES = [\"train\", \"test\", \"val\"]\n",
    "DOWNSIZE_SIZE = 3\n",
    "SHUFFLE_BEFORE_SAVING = True\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_USED_LABELS = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 17, 18, 19, 23, 27, 31, 33, 34]\n",
    "IMAGE_MAX_LABELS = 35\n",
    "IMG_WIDTH = 1920\n",
    "IMG_HEIGHT = 1200\n",
    "\n",
    "DEPTH_MAX = 20\n",
    "\n",
    "splits = {subset_name: [] for subset_name in IMAGE_SPLIT_FILES}\n",
    "\n",
    "for subset_name in IMAGE_SPLIT_FILES:    \n",
    "    with open(\"Rellis-3D/%s.lst\" % subset_name) as f:\n",
    "        for l in f.readlines():\n",
    "            split = int(l.split(\"/\")[0].strip())\n",
    "            files = [\"Rellis-3D/%s\" % img for img in l.split(\" \")]\n",
    "            \n",
    "            splits[subset_name].append((split, files[0].strip(), files[1].strip()))\n",
    "    \n",
    "    new_split = {img[0]: [] for img in splits[subset_name] + [(-1,)]}\n",
    "    if SHUFFLE_BEFORE_SAVING:\n",
    "        shuffle(splits[subset_name])\n",
    "    \n",
    "    all_imgs = splits[subset_name]\n",
    "    #splits[subset_name].extend([(-1, i[1], i[2]) for i in all_imgs])\n",
    "    \n",
    "    for img in splits[subset_name]:\n",
    "        new_split[img[0]].append((img[1], img[2]))\n",
    "    splits[subset_name] = new_split\n",
    "\n",
    "for subset_name in IMAGE_SPLIT_FILES:\n",
    "    for split_name, split in splits[subset_name].items():\n",
    "        if split_name != -1:\n",
    "            splits[subset_name][split_name] = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baea344a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T02:14:34.537941Z",
     "iopub.status.busy": "2023-05-29T02:14:34.536832Z",
     "iopub.status.idle": "2023-05-29T02:14:34.660185Z",
     "shell.execute_reply": "2023-05-29T02:14:34.658886Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add ply data\n",
    "import re\n",
    "\n",
    "search_1 = r\"Rellis-3D/\\d\\d\\d\\d\\d\"\n",
    "search_2 = r\"\\d\\d\\d\\d\\d\\d(?=[-])\"\n",
    "os_folder = \"/os1_cloud_node_kitti_bin/\"\n",
    "vel_folder = \"/vel_cloud_node_kitti_bin/\"\n",
    "\n",
    "def os_ply_name(img_name):\n",
    "    return re.findall(search_1, img_name)[0] + os_folder + re.findall(search_2, img_name)[-1] + \".bin\"\n",
    "\n",
    "def vel_ply_name(img_name):\n",
    "    return re.findall(search_1, img_name)[0] + vel_folder + re.findall(search_2, img_name)[-1] + \".bin\"\n",
    "\n",
    "for subset_name in IMAGE_SPLIT_FILES:\n",
    "    for split_name, split in splits[subset_name].items():\n",
    "        splits[subset_name][split_name] = [\n",
    "            {\n",
    "                \"i\": i,\n",
    "                \"img\": \"/home/ian/Rellis/\" + files[0],\n",
    "                \"img_segmented\": \"/home/ian/Rellis/\" + files[1],\n",
    "                \"cam_intrinsic\": \"/home/ian/Rellis/\" + re.findall(search_1, files[0])[0] + \"/camera_info.txt\",\n",
    "                \"os_transform\": \"/home/ian/Rellis/\" + re.findall(search_1, files[0])[0] + \"/transforms.yaml\",\n",
    "                \"os_ply_file\": \"/home/ian/Rellis/\" + os_ply_name(files[0]),\n",
    "                \"vel_transform\": \"/home/ian/Rellis/\" + re.findall(search_1, files[0])[0] + \"/vel2os1.yaml\",\n",
    "                \"vel_ply_file\": \"/home/ian/Rellis/\" + vel_ply_name(files[0]),\n",
    "            } for i, files in enumerate(splits[subset_name][split_name])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b634a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T02:14:34.665519Z",
     "iopub.status.busy": "2023-05-29T02:14:34.665117Z",
     "iopub.status.idle": "2023-05-29T02:14:34.674573Z",
     "shell.execute_reply": "2023-05-29T02:14:34.673626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0,\n",
       " 'img': '/home/ian/Rellis/Rellis-3D/00000/pylon_camera_node/frame000941-1581624746_850.jpg',\n",
       " 'img_segmented': '/home/ian/Rellis/Rellis-3D/00000/pylon_camera_node_label_id/frame000941-1581624746_850.png',\n",
       " 'cam_intrinsic': '/home/ian/Rellis/Rellis-3D/00000/camera_info.txt',\n",
       " 'os_transform': '/home/ian/Rellis/Rellis-3D/00000/transforms.yaml',\n",
       " 'os_ply_file': '/home/ian/Rellis/Rellis-3D/00000/os1_cloud_node_kitti_bin/000941.bin',\n",
       " 'vel_transform': '/home/ian/Rellis/Rellis-3D/00000/vel2os1.yaml',\n",
       " 'vel_ply_file': '/home/ian/Rellis/Rellis-3D/00000/vel_cloud_node_kitti_bin/000941.bin'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[\"train\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc0fa99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T02:14:34.683158Z",
     "iopub.status.busy": "2023-05-29T02:14:34.682677Z",
     "iopub.status.idle": "2023-05-29T02:14:34.944949Z",
     "shell.execute_reply": "2023-05-29T02:14:34.944233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for projections\n",
    "# From: https://github.com/unmannedlab/RELLIS-3D/blob/main/utils/lidar2img.ipynb\n",
    "import yaml\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "\n",
    "def load_from_bin(bin_path):\n",
    "    obj = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4)\n",
    "    # ignore reflectivity info\n",
    "    return obj[:,:3]\n",
    "\n",
    "\n",
    "def points_filter(points,img_width,img_height,P,RT):\n",
    "    ctl = RT\n",
    "    ctl = np.array(ctl)\n",
    "    fov_x = 2*np.arctan2(img_width, 2*P[0,0])*180/3.1415926+10\n",
    "    fov_y = 2*np.arctan2(img_height, 2*P[1,1])*180/3.1415926+10\n",
    "    R= np.eye(4)\n",
    "    p_l = np.ones((points.shape[0],points.shape[1]+1))\n",
    "    p_l[:,:3] = points\n",
    "    p_c = np.matmul(ctl,p_l.T)\n",
    "    p_c = p_c.T\n",
    "    x = p_c[:,0]\n",
    "    y = p_c[:,1]\n",
    "    z = p_c[:,2]\n",
    "    dist = np.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "    xangle = np.arctan2(x, z)*180/np.pi;\n",
    "    yangle = np.arctan2(y, z)*180/np.pi;\n",
    "    flag2 = (xangle > -fov_x/2) & (xangle < fov_x/2)\n",
    "    flag3 = (yangle > -fov_y/2) & (yangle < fov_y/2)\n",
    "    res = p_l[flag2&flag3,:3]\n",
    "    res = np.array(res)\n",
    "    x = res[:, 0]\n",
    "    y = res[:, 1]\n",
    "    z = res[:, 2]\n",
    "    dist = np.sqrt(x ** 2 + y ** 2 + z ** 2)\n",
    "    return res,dist\n",
    "\n",
    "\n",
    "def get_cam_mtx(filepath):\n",
    "    data = np.loadtxt(filepath)\n",
    "    P = np.zeros((3,3))\n",
    "    P[0,0] = data[0]\n",
    "    P[1,1] = data[1]\n",
    "    P[2,2] = 1\n",
    "    P[0,2] = data[2]\n",
    "    P[1,2] = data[3]\n",
    "    return P\n",
    "\n",
    "\n",
    "def get_mtx_from_yaml(filepath,key='os1_cloud_node-pylon_camera_node'):\n",
    "    with open(filepath,'r') as f:\n",
    "        data = yaml.load(f,Loader= yaml.Loader)\n",
    "    q = data[key]['q']\n",
    "    q = np.array([q['x'],q['y'],q['z'],q['w']])\n",
    "    t = data[key]['t']\n",
    "    t = np.array([t['x'],t['y'],t['z']])\n",
    "    R_vc = Rotation.from_quat(q)\n",
    "    R_vc = R_vc.as_matrix()\n",
    "\n",
    "    RT = np.eye(4,4)\n",
    "    RT[:3,:3] = R_vc\n",
    "    RT[:3,-1] = t\n",
    "    RT = np.linalg.inv(RT)\n",
    "    return RT\n",
    "\n",
    "\n",
    "distCoeff = np.array([-0.134313,-0.025905,0.002181,0.00084,0])\n",
    "distCoeff = distCoeff.reshape((5,1))\n",
    "\n",
    "\n",
    "def load_os_points(tranform_file, ply_file, P):\n",
    "    points = load_from_bin(ply_file)\n",
    "    \n",
    "    RT = get_mtx_from_yaml(tranform_file)\n",
    "    R_vc = RT[:3,:3]\n",
    "    T_vc = RT[:3,3]\n",
    "    T_vc = T_vc.reshape(3, 1)\n",
    "    rvec,_ = cv2.Rodrigues(R_vc)\n",
    "    tvec = T_vc\n",
    "    xyz_v, dist = points_filter(points,IMG_WIDTH,IMG_HEIGHT,P,RT)\n",
    "    imgpoints, _ = cv2.projectPoints(xyz_v[:,:],rvec, tvec, P, distCoeff)\n",
    "    imgpoints = np.squeeze(imgpoints,1)\n",
    "    return imgpoints.T, dist\n",
    "\n",
    "\n",
    "def load_vel_points(tranform_file, os_transform_file, ply_file, P):\n",
    "    RT = get_mtx_from_yaml(os_transform_file)\n",
    "    R_vc = RT[:3,:3]\n",
    "    T_vc = RT[:3,3]\n",
    "    T_vc = T_vc.reshape(3, 1)\n",
    "    rvec,_ = cv2.Rodrigues(R_vc)\n",
    "    tvec = T_vc\n",
    "    \n",
    "    vel2os = get_mtx_from_yaml(tranform_file, 'vel2os1')\n",
    "    \n",
    "    velpoints = load_from_bin(ply_file)\n",
    "    \n",
    "    velpcd_ = np.ones((velpoints.shape[0],4))\n",
    "    velpcd_[:,:3] = velpoints\n",
    "    velpcdos = vel2os@velpcd_.T\n",
    "    velpcdos = velpcdos.T[:,:3]\n",
    "    \n",
    "    xyz_v, c_ = points_filter(velpcdos,IMG_WIDTH,IMG_HEIGHT,P,RT)\n",
    "\n",
    "    imgpoints, _ = cv2.projectPoints(xyz_v[:,:],rvec, tvec, P, distCoeff)\n",
    "    imgpoints = np.squeeze(imgpoints,1)\n",
    "    return imgpoints.T, c_\n",
    "\n",
    "\n",
    "def add_depth_axis(img, points, depths):\n",
    "    if img.shape[-1] == 3:\n",
    "        img = np.insert(img, 3, 0, axis=-1)\n",
    "    \n",
    "    for i in range(points.shape[1]):\n",
    "        if IMG_HEIGHT - 1 > points[1][i] > 0 and IMG_WIDTH - 1 > points[0][i] > 0:\n",
    "            img[np.int32(points[1][i]), np.int32(points[0][i]), -1] = np.float16(depths[i]/DEPTH_MAX)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebeffaae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T02:14:34.948726Z",
     "iopub.status.busy": "2023-05-29T02:14:34.948277Z",
     "iopub.status.idle": "2023-05-29T02:14:35.215982Z",
     "shell.execute_reply": "2023-05-29T02:14:35.215149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_dot_img(img_with_depth):\n",
    "    dots = []\n",
    "    for y in range(img_with_depth.shape[0]):\n",
    "        for x in range(img_with_depth.shape[1]):\n",
    "            depth = img_with_depth[y, x, 3]\n",
    "            if depth > 0:\n",
    "                dots.append((x, y, depth))\n",
    "    img_without_depth = img_with_depth[:, :, 0:3]\n",
    "    plt.imshow(img_without_depth)\n",
    "    color = [d[2] for d in dots]\n",
    "    plt.scatter([d[0] for d in dots], [d[1] for d in dots], s=1, c=color, cmap=\"plasma\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_image(img):\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549a44f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T02:14:35.221257Z",
     "iopub.status.busy": "2023-05-29T02:14:35.221065Z",
     "iopub.status.idle": "2023-05-29T13:41:20.879277Z",
     "shell.execute_reply": "2023-05-29T13:41:20.868918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples, split 0\n",
      "train samples, split 2\n",
      "train samples, split 3\n",
      "train samples, split 4\n",
      "test samples, split 0\n",
      "test samples, split 1\n",
      "test samples, split 2\n",
      "File not found\n",
      "File not found\n",
      "File not found\n",
      "File not found\n",
      "val samples, split 0\n",
      "val samples, split 1\n",
      "File not found\n",
      "File not found\n",
      "File not found\n"
     ]
    }
   ],
   "source": [
    "# Load image files and labels\n",
    "# Save data\n",
    "from threading import Thread\n",
    "from os import remove\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.measure import block_reduce\n",
    "from npy_append_array import NpyAppendArray\n",
    "\n",
    "\n",
    "def convert_img_to_labels(labeled_img_file_name):\n",
    "    img_labels = np.asarray(Image.open(labeled_img_file_name))\n",
    "    img_labels_new = np.zeros(img_labels.shape + (len(IMAGE_USED_LABELS),))\n",
    "    for i, label in enumerate(IMAGE_USED_LABELS):\n",
    "        img_labels_new[:,:,i] = (img_labels[:,:] == label)\n",
    "    return img_labels_new\n",
    "\n",
    "\n",
    "def load_img(img_file_name):\n",
    "    img_base = np.asarray(Image.open(img_file_name))\n",
    "    img_base = img_base / 255\n",
    "    return img_base\n",
    "\n",
    "\n",
    "def add_depth_to_img(img, cam_intrinsic_file, os_calib, os_points, vel_calib, vel_points):\n",
    "    P = get_cam_mtx(cam_intrinsic_file)\n",
    "\n",
    "    points, depths = load_os_points(os_calib, os_points, P)\n",
    "    img = add_depth_axis(img, points, depths)\n",
    "\n",
    "    points, depths = load_vel_points(vel_calib, os_calib, vel_points, P)\n",
    "    \n",
    "    img = add_depth_axis(img, points, depths)\n",
    "    return img\n",
    "\n",
    "\n",
    "def downsize(img):    \n",
    "    return block_reduce(img, (DOWNSIZE_SIZE, DOWNSIZE_SIZE, 1), np.max)\n",
    "\n",
    "\n",
    "def append_array(np_array_file, np_array):\n",
    "    np_array = np.expand_dims(np_array, 0)\n",
    "    try:\n",
    "        np_array_file.append(np_array)\n",
    "    except ValueError:\n",
    "        np_array_file.recover()\n",
    "        np_array_file.append(np_array)\n",
    "\n",
    "\n",
    "def create_data(img_paths):\n",
    "    try:\n",
    "        new_imgs = {}\n",
    "        new_imgs[\"img\"] = load_img(img_paths[\"img\"])\n",
    "        img = new_imgs[\"img\"].astype(np.float16)\n",
    "        append_array(numpy_files[0], img)\n",
    "        append_array(all_numpy_files[0], img)\n",
    "\n",
    "        new_imgs[\"img_with_depth\"] = add_depth_to_img(new_imgs[\"img\"], img_paths[\"cam_intrinsic\"], img_paths[\"os_transform\"],\n",
    "            img_paths[\"os_ply_file\"], img_paths[\"vel_transform\"], img_paths[\"vel_ply_file\"])\n",
    "        img = new_imgs[\"img\"].astype(np.float16)\n",
    "        append_array(numpy_files[2], img)\n",
    "        append_array(all_numpy_files[2], img)\n",
    "\n",
    "        new_imgs[\"img_segmented\"] = convert_img_to_labels(img_paths[\"img_segmented\"])\n",
    "        img = new_imgs[\"img\"].astype(np.uint8)\n",
    "        append_array(numpy_files[4], img)\n",
    "        append_array(all_numpy_files[4], img)\n",
    "\n",
    "        create_downsized(new_imgs)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found\")\n",
    "\n",
    "\n",
    "def create_downsized(imgs):\n",
    "    new_imgs = {}\n",
    "    img = downsize(imgs[\"img\"]).astype(np.float16)\n",
    "    append_array(numpy_files[1], img)\n",
    "    append_array(all_numpy_files[1], img)\n",
    "    \n",
    "    img = downsize(imgs[\"img_with_depth\"]).astype(np.float16)\n",
    "    append_array(numpy_files[3], img)\n",
    "    append_array(all_numpy_files[3], img)\n",
    "    \n",
    "    img = downsize(imgs[\"img_segmented\"]).astype(np.float16)\n",
    "    append_array(numpy_files[5], img)\n",
    "    append_array(all_numpy_files[5], img)\n",
    "    \n",
    "\n",
    "columns_to_save = [\n",
    "    (\"img\", np.float16),\n",
    "    (\"img_ds\", np.float16),\n",
    "    (\"img_with_depth\", np.float16),\n",
    "    (\"img_with_depth_ds\", np.float16),\n",
    "    (\"img_one_hot_labels\", np.uint8),\n",
    "    (\"img_one_hot_labels_ds\", np.uint8),\n",
    "]\n",
    "\n",
    "for subset_name, subset in splits.items():\n",
    "    all_numpy_files = [NpyAppendArray(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, col[0])) for col in columns_to_save]\n",
    "    for split_name, split in subset.items():\n",
    "        if split_name == -1:\n",
    "            continue\n",
    "        numpy_files = [NpyAppendArray(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, col[0])) for col in columns_to_save]\n",
    "        print(\"%s samples, split %d\" % (subset_name, split_name))\n",
    "        \n",
    "        threads = [Thread(target=create_data, args=(s,)) for s in split]\n",
    "        [t.run() for t in threads]\n",
    "        \n",
    "        np.savez_compressed(\"Processed Data/%s_%d\" % (subset_name, split_name),\n",
    "            img=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img\")),\n",
    "            img_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_ds\")),\n",
    "            img_depth=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_with_depth\")),\n",
    "            img_depth_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_with_depth_ds\")),\n",
    "            img_oh=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_one_hot_labels\")),\n",
    "            img_oh_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_one_hot_labels_ds\")),\n",
    "        )\n",
    "        for f in numpy_files:\n",
    "            remove(f.filename)\n",
    "\n",
    "    np.savez_compressed(\"Processed Data/%s_%d\" % (subset_name, -1),\n",
    "        img=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, \"img\")),\n",
    "        img_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, \"img_ds\")),\n",
    "        img_depth=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, \"img_with_depth\")),\n",
    "        img_depth_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, \"img_with_depth_ds\")),\n",
    "        img_oh=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, \"img_one_hot_labels\")),\n",
    "        img_oh_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, \"img_one_hot_labels_ds\")),\n",
    "    )\n",
    "    for f in all_numpy_files:\n",
    "        remove(f.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8c012d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T13:41:20.911637Z",
     "iopub.status.busy": "2023-05-29T13:41:20.909435Z",
     "iopub.status.idle": "2023-05-29T13:41:20.939565Z",
     "shell.execute_reply": "2023-05-29T13:41:20.938690Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Code to break up images that were accidently appended together\n",
    "# columns_to_save = [\n",
    "#     (\"img\", \"np.float16\"),\n",
    "#     (\"img_ds\", \"np.float16\"),\n",
    "#     (\"img_depth\", \"np.float16\"),\n",
    "#     (\"img_depth_ds\", \"np.float16\"),\n",
    "#     (\"img_oh\", \"np.uint8\"),\n",
    "#     (\"img_oh_ds\", \"np.uint8\"),\n",
    "# ]\n",
    "\n",
    "# for name in [\"train\", \"val\", \"test\"]:\n",
    "#     for split in [0, 2, 3, 4, -1]:\n",
    "#         vals_to_save = []\n",
    "#         with np.load(\"Processed Data/%s_%d.npz\" % (name, split)) as data:\n",
    "#             for c, t in columns_to_save:\n",
    "#                 exec(\"%s=data['%s']\" % (c, c))\n",
    "#                 if c.endswith(\"ds\"):\n",
    "#                     l = eval(\"%s\" % c).shape[0]//400\n",
    "#                 else:\n",
    "#                     l = eval(\"%s\" % c).shape[0]//1200\n",
    "#                 exec(\"%s=np.split(%s, l)\" % (c, c))\n",
    "#                 vals_to_save.append(\"%s=np.array(%s).astype(%s)\" % (c, c, t))\n",
    "#         exec(\"np.savez_compressed('Processed Data/%s_%d', %s)\" % (name, split, \",\".join(vals_to_save)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d4f9b",
   "metadata": {},
   "source": [
    "# Old code that doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f746f5be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T13:41:20.947003Z",
     "iopub.status.busy": "2023-05-29T13:41:20.946491Z",
     "iopub.status.idle": "2023-05-29T13:41:20.960514Z",
     "shell.execute_reply": "2023-05-29T13:41:20.959377Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Load image files and labels\n",
    "# # Save data\n",
    "# from multiprocessing import Pool\n",
    "# from os import remove\n",
    "# import time\n",
    "\n",
    "# import numpy as np\n",
    "# from PIL import Image\n",
    "# from skimage.measure import block_reduce\n",
    "# from npy_append_array import NpyAppendArray\n",
    "\n",
    "\n",
    "# def convert_img_to_labels(labeled_img_file_name):\n",
    "#     img_labels = np.asarray(Image.open(labeled_img_file_name))\n",
    "#     img_labels_new = np.zeros(img_labels.shape + (len(IMAGE_USED_LABELS),))\n",
    "#     for i, label in enumerate(IMAGE_USED_LABELS):\n",
    "#         img_labels_new[:,:,i] = (img_labels[:,:] == label)\n",
    "#     return img_labels_new\n",
    "\n",
    "\n",
    "# def load_img(img_file_name):\n",
    "#     img_base = np.asarray(Image.open(img_file_name))\n",
    "#     img_base = img_base / 255\n",
    "#     return img_base\n",
    "\n",
    "\n",
    "# def add_depth_to_img(img, cam_intrinsic_file, os_calib, os_points, vel_calib, vel_points):\n",
    "#     P = get_cam_mtx(cam_intrinsic_file)\n",
    "\n",
    "#     points, depths = load_os_points(os_calib, os_points, P)\n",
    "#     img = add_depth_axis(img, points, depths)\n",
    "\n",
    "#     points, depths = load_vel_points(vel_calib, os_calib, vel_points, P)\n",
    "    \n",
    "#     img = add_depth_axis(img, points, depths)\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def downsize(img):    \n",
    "#     return block_reduce(img, (DOWNSIZE_SIZE, DOWNSIZE_SIZE, 1), np.max)\n",
    "\n",
    "\n",
    "# def create_data(img_paths):\n",
    "#     new_imgs = {}\n",
    "#     new_imgs[\"img\"] = load_img(img_paths[\"img\"])\n",
    "#     new_imgs[\"img_segmented\"] = convert_img_to_labels(img_paths[\"img_segmented\"])\n",
    "#     new_imgs[\"img_with_depth\"] = add_depth_to_img(new_imgs[\"img\"], img_paths[\"cam_intrinsic\"], img_paths[\"os_transform\"],\n",
    "#         img_paths[\"os_ply_file\"], img_paths[\"vel_transform\"], img_paths[\"vel_ply_file\"])\n",
    "#     return new_imgs\n",
    "\n",
    "\n",
    "# def create_downsized(imgs):\n",
    "#     new_imgs = {}\n",
    "#     new_imgs[\"img\"] = downsize(imgs[\"img\"])\n",
    "#     new_imgs[\"img_segmented\"] = downsize(imgs[\"img_segmented\"])\n",
    "#     new_imgs[\"img_with_depth\"] = downsize(imgs[\"img_with_depth\"])\n",
    "#     return new_imgs\n",
    "    \n",
    "\n",
    "# columns_to_save = [\n",
    "#     (\"img\", np.float16),\n",
    "#     (\"img_ds\", np.float16),\n",
    "#     (\"img_with_depth\", np.float16),\n",
    "#     (\"img_with_depth_ds\", np.float16),\n",
    "#     (\"img_one_hot_labels\", np.uint8),\n",
    "#     (\"img_one_hot_labels_ds\", np.uint8),\n",
    "# ]\n",
    "\n",
    "# for subset_name, subset in splits.items():\n",
    "#     all_numpy_files = [NpyAppendArray(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, -1, col[0])) for col in columns_to_save]\n",
    "#     for split_name, split in subset.items():\n",
    "#         numpy_files = [NpyAppendArray(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, col[0])) for col in columns_to_save]\n",
    "#         print(\"%s samples, split %d\" % (subset_name, split_name))\n",
    "        \n",
    "#         split = [split[i:i+BATCH_SIZE] for i in range(0, len(split), BATCH_SIZE)]\n",
    "#         with Pool(BATCH_SIZE//8) as p:\n",
    "#             for i, s in enumerate(split):\n",
    "#                 s = p.map(create_data, s)\n",
    "\n",
    "#                 img = np.concatenate([r[\"img\"] for r in s]).astype(np.float16)\n",
    "#                 numpy_files[0].append(img)\n",
    "#                 all_numpy_files[0].append(img)\n",
    "\n",
    "#                 img = np.concatenate([r[\"img_with_depth\"] for r in s]).astype(np.float16)\n",
    "#                 numpy_files[2].append(img)\n",
    "#                 all_numpy_files[2].append(img)\n",
    "\n",
    "#                 img = np.concatenate([r[\"img_segmented\"] for r in s]).astype(np.uint8)\n",
    "#                 numpy_files[4].append(img)\n",
    "#                 all_numpy_files[4].append(img)\n",
    "\n",
    "#                 if i == 0:\n",
    "#                     draw_image(s[0][\"img\"])\n",
    "#                     draw_dot_img(s[0][\"img_with_depth\"])\n",
    "\n",
    "#                 s = p.map(create_downsized, s)\n",
    "\n",
    "#                 img = np.concatenate([r[\"img\"] for r in s]).astype(np.float16)\n",
    "#                 numpy_files[1].append(img)\n",
    "#                 all_numpy_files[1].append(img)\n",
    "\n",
    "#                 img = np.concatenate([r[\"img_with_depth\"] for r in s]).astype(np.float16)\n",
    "#                 numpy_files[3].append(img)\n",
    "#                 all_numpy_files[3].append(img)\n",
    "\n",
    "#                 img = np.concatenate([r[\"img_segmented\"] for r in s]).astype(np.uint8)\n",
    "#                 numpy_files[5].append(img)\n",
    "#                 all_numpy_files[5].append(img)\n",
    "\n",
    "#                 if i == 0:\n",
    "#                     draw_image(s[0][\"img\"])\n",
    "#                     draw_dot_img(s[0][\"img_with_depth\"])\n",
    "\n",
    "#                 print(\"%.2f%% Complete\" % ((i+1) * 100/len(split)))\n",
    "#         numpy.savez_compressed(\"Processed Data/rellis_%s_%d.npy\" % (subset_name, split_name),\n",
    "#             img=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img\")),\n",
    "#             img_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_ds\")),\n",
    "#             img_depth=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_with_depth\")),\n",
    "#             img_depth_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_with_depth_ds\")),\n",
    "#             img_oh=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_one_hot_labels\")),\n",
    "#             img_oh_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_one_hot_labels_ds\")),\n",
    "#         )\n",
    "#         for f in numpy_files:\n",
    "#             remove(f)\n",
    "\n",
    "#     split_name = -1\n",
    "#     numpy.savez_compressed(\"Processed Data/rellis_%s_%d.npy\" % (subset_name, split_name),\n",
    "#         img=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img\")),\n",
    "#         img_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_ds\")),\n",
    "#         img_depth=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_with_depth\")),\n",
    "#         img_depth_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_with_depth_ds\")),\n",
    "#         img_oh=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_one_hot_labels\")),\n",
    "#         img_oh_ds=np.load(\"Processed Data/images_%s_%d_%s.npy\" % (subset_name, split_name, \"img_one_hot_labels_ds\")),\n",
    "#     )\n",
    "#     for f in all_numpy_files:\n",
    "#         remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fe9266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T13:41:20.968840Z",
     "iopub.status.busy": "2023-05-29T13:41:20.968450Z",
     "iopub.status.idle": "2023-05-29T13:41:20.975885Z",
     "shell.execute_reply": "2023-05-29T13:41:20.974761Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Load image files and labels\n",
    "# # Save data\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import ArrayType, FloatType, IntegerType\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# from pyspark.sql import functions\n",
    "# import os\n",
    "\n",
    "# from PIL import Image\n",
    "# from skimage.measure import block_reduce\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from npy_append_array import NpyAppendArray\n",
    "\n",
    "\n",
    "# def convert_img_to_labels(labeled_img_file_name):\n",
    "#     img_labels = np.asarray(Image.open(labeled_img_file_name))\n",
    "#     img_labels_new = np.zeros(img_labels.shape + (IMAGE_MAX_LABELS,))\n",
    "#     for i in range(IMAGE_MAX_LABELS):\n",
    "#         if i in IMAGE_USED_LABELS:\n",
    "#             img_labels_new[:,:,i] = (img_labels[:,:] == i)\n",
    "#     shape = img_base.shape\n",
    "#     return Vectors.dense((img_labels_new.astype(\"int\").reshape(shape[0] * shape[1] * shape[2])))\n",
    "# convert_img_to_labels_udf = functions.udf(convert_img_to_labels, VectorUDT())\n",
    "\n",
    "\n",
    "# def load_img(img_file_name):\n",
    "#     print(\"Reading file\")\n",
    "#     img_base = np.asarray(Image.open(img_file_name))\n",
    "#     img_base = img_base / 255\n",
    "    \n",
    "#     shape = img_base.shape\n",
    "#     return Vectors.dense(img_base.astype(\"float\").reshape(shape[0] * shape[1] * shape[2]))\n",
    "# load_img_udf = functions.udf(load_img, VectorUDT())\n",
    "\n",
    "\n",
    "# def add_depth_to_img(img, cam_intrinsic_file, os_calib, os_points, vel_calib, vel_points):\n",
    "#     P = get_cam_mtx(cam_intrinsic_file)\n",
    "\n",
    "#     points, depths = load_os_points(os_calib, os_points, P)\n",
    "#     img = add_depth_axis(img, points, depths)\n",
    "\n",
    "#     points, depths = load_vel_points(vel_calib, vel_points, P)\n",
    "    \n",
    "#     img = img.toArray()\n",
    "#     channels = img.shape[0]//(IMG_WIDTH * IMG_HEIGHT)\n",
    "#     img = img.reshape(IMG_WIDTH, IMG_HEIGHT, channels)\n",
    "#     img = add_depth_axis(img, points, depths)\n",
    "    \n",
    "#     shape = img_base.shape\n",
    "#     return Vectors.dense(img.astype(\"float\").reshape(shape[0] * shape[1] * shape[2]))\n",
    "# add_depth_to_img_udf = functions.udf(add_depth_to_img, VectorUDT())\n",
    "\n",
    "\n",
    "# def downsize(img):\n",
    "#     channels = img.shape[0]//(IMG_WIDTH * IMG_HEIGHT)\n",
    "#     img = img.reshape(IMG_WIDTH, IMG_HEIGHT, channels)\n",
    "    \n",
    "#     img = block_reduce(img, (DOWNSIZE_SIZE, DOWNSIZE_SIZE, 1), np.max)\n",
    "    \n",
    "#     shape = img_base.shape\n",
    "#     return Vectors.dense(img.reshape(shape[0] * shape[1] * shape[2]))\n",
    "# downsize_udf = functions.udf(downsize, VectorUDT())\n",
    "\n",
    "\n",
    "# # Process images in spark\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/ian/miniconda3/envs/SparseIVA/bin/python3.9'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/ian/miniconda3/envs/SparseIVA/bin/python3.9'\n",
    "\n",
    "# # spark = SparkSession.builder.master(\"spark://147.9.188.154:7077\").appName(\"Rellis\") \n",
    "# spark = SparkSession.builder.master(\"local[1]\").appName(\"Rellis\")\n",
    "# spark.config('spark.driver.memory','16g')\n",
    "# spark.config('spark.executor.memory','8g')\n",
    "# spark.config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# spark.config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"true\")\n",
    "# spark = spark.getOrCreate()\n",
    "\n",
    "# columns_to_save = [\n",
    "#     (\"img_loaded\", np.float16),\n",
    "#     (\"img_loaded_ds\", np.float16),\n",
    "#     (\"img_with_depth\", np.float16),\n",
    "#     (\"img_with_depth_ds\", np.float16),\n",
    "#     (\"img_one_hot_labels\", np.uint8),\n",
    "#     (\"img_one_hot_labels_ds\", np.uint8),\n",
    "# ]\n",
    "\n",
    "# for subset_name, subset in splits.items():\n",
    "#     for split_name, split in subset.items():\n",
    "#         numpy_files = [NpyAppendArray(\"Processed Data/images_%s_%d_%s.npz\" % (subset_name, split_name, col[0])) for col in columns_to_save]\n",
    "        \n",
    "#         data = spark.createDataFrame(pd.DataFrame(split))\n",
    "#         num_rows = data.count()\n",
    "\n",
    "#         data = data.withColumn(\"img_loaded\", load_img_udf(data[\"img\"]))\n",
    "#         data = data.withColumn(\"img_with_depth\", add_depth_to_img_udf(data[\"img_loaded\"],\n",
    "#               data[\"os_transform\"], data[\"os_ply_file\"], data[\"vel_transform\"], data[\"vel_ply_file\"]\n",
    "#         ))\n",
    "        \n",
    "#         data = data.withColumn(\"img_one_hot_labels\", convert_img_to_labels_udf(data[\"img_segmented\"]))\n",
    "        \n",
    "#         data = data.withColumn(\"img_loaded_ds\", downsize_udf(data[\"img_loaded\"]))\n",
    "#         data = data.withColumn(\"img_with_depth_ds\", downsize_udf(data[\"img_with_depth\"]))\n",
    "#         data = data.withColumn(\"img_one_hot_labels_ds\", downsize_udf(data[\"img_one_hot_labels\"]))\n",
    "        \n",
    "#         print(subset_name, split_name)\n",
    "        \n",
    "#         for i in range(0, num_rows, 1):\n",
    "#             smaller_data = data.filter(data.i.between(i, i))\n",
    "#             for col, np_file in zip(columns_to_save, numpy_files):\n",
    "#                 np_file.append(smaller_data.select(col[0]).collect()[0][0].toArray().astype(col[1]))\n",
    "\n",
    "# # def load_img(imgs):\n",
    "# #     img_base = np.asarray(Image.open(imgs[0])).astype(np.uint8)\n",
    "\n",
    "# #     img_labels = np.asarray(Image.open(imgs[1]))\n",
    "# #     img_labels_new = np.zeros(img_labels.shape + (IMAGE_MAX_LABELS,))\n",
    "# #     for i in range(IMAGE_MAX_LABELS):\n",
    "# #         if i in IMAGE_USED_LABELS:\n",
    "# #             img_labels_new[:,:,i] = (img_labels[:,:] == i)\n",
    "\n",
    "# #     return (img_base, img_labels_new.astype(np.uint8))\n",
    "\n",
    "\n",
    "# # for subset_name, subset in splits.items():\n",
    "# #     for split_name, split in subset.items():\n",
    "# #         with Pool(64) as p:\n",
    "# #             new_split = p.map(load_img, split)\n",
    "            \n",
    "# # #         if SHUFFLE_BEFORE_SAVING:\n",
    "# # #             shuffle(new_split)\n",
    "                    \n",
    "# #         dump(new_split, open(\"Processed Data/images_%s_%d.pickle\" % (subset_name, split_name), \"wb+\"))\n",
    "# #         print(subset_name, split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60901a4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T13:41:20.983313Z",
     "iopub.status.busy": "2023-05-29T13:41:20.983050Z",
     "iopub.status.idle": "2023-05-29T13:41:20.992696Z",
     "shell.execute_reply": "2023-05-29T13:41:20.991973Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://dl.acm.org/doi/pdf/10.5555/2830840.2830844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4671aa7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-29T13:41:20.997772Z",
     "iopub.status.busy": "2023-05-29T13:41:20.997622Z",
     "iopub.status.idle": "2023-05-29T13:41:21.000940Z",
     "shell.execute_reply": "2023-05-29T13:41:21.000220Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load ply data\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c2ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparseIVA",
   "language": "python",
   "name": "sparseiva"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
